{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a03d8b",
   "metadata": {},
   "source": [
    "# Theory: Importing Required Libraries\n",
    "\n",
    "Before running prompt debugging and evaluation metrics, you need to install the necessary Python libraries:\n",
    "\n",
    "- `sacrebleu`: For BLEU score calculation.\n",
    "- `rouge_score`: For ROUGE metrics.\n",
    "- `sentence_transformers`: For semantic similarity.\n",
    "\n",
    "The following code cell installs these packages using pip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad37047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: regex in e:\\mca\\sem3\\genai\\lab\\.venv\\lib\\site-packages (from sacrebleu) (2025.9.1)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\mca\\sem3\\genai\\lab\\.venv\\lib\\site-packages (from sacrebleu) (2.3.3)\n",
      "Requirement already satisfied: colorama in e:\\mca\\sem3\\genai\\lab\\.venv\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-6.0.2-cp313-cp313-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pywin32>=226 in e:\\mca\\sem3\\genai\\lab\\.venv\\lib\\site-packages (from portalocker->sacrebleu) (311)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading lxml-6.0.2-cp313-cp313-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/4.0 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.3/4.0 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.1/4.0 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.4/4.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 3.6 MB/s  0:00:01\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: tabulate, portalocker, lxml, sacrebleu\n",
      "\n",
      "   ---------------------------------------- 0/4 [tabulate]\n",
      "   ---------- ----------------------------- 1/4 [portalocker]\n",
      "   ---------- ----------------------------- 1/4 [portalocker]\n",
      "   -------------------- ------------------- 2/4 [lxml]\n",
      "   -------------------- ------------------- 2/4 [lxml]\n",
      "   -------------------- ------------------- 2/4 [lxml]\n",
      "   -------------------- ------------------- 2/4 [lxml]\n",
      "   -------------------- ------------------- 2/4 [lxml]\n",
      "   -------------------- ------------------- 2/4 [lxml]\n",
      "   -------------------- ------------------- 2/4 [lxml]\n",
      "   -------------------- ------------------- 2/4 [lxml]\n",
      "   ------------------------------ --------- 3/4 [sacrebleu]\n",
      "   ------------------------------ --------- 3/4 [sacrebleu]\n",
      "   ------------------------------ --------- 3/4 [sacrebleu]\n",
      "   ------------------------------ --------- 3/4 [sacrebleu]\n",
      "   ------------------------------ --------- 3/4 [sacrebleu]\n",
      "   ------------------------------ --------- 3/4 [sacrebleu]\n",
      "   ------------------------------ --------- 3/4 [sacrebleu]\n",
      "   ------------------------------ --------- 3/4 [sacrebleu]\n",
      "   ------------------------------ --------- 3/4 [sacrebleu]\n",
      "   ---------------------------------------- 4/4 [sacrebleu]\n",
      "\n",
      "Successfully installed lxml-6.0.2 portalocker-3.2.0 sacrebleu-2.5.1 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu rouge_score sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c62335e",
   "metadata": {},
   "source": [
    "# Theory: Prompt Debugging\n",
    "\n",
    "Prompt debugging involves generating variations of prompts and testing their outputs. This helps in identifying the most effective prompt for a given task.\n",
    "\n",
    "- `generate_variations`: Creates all possible combinations of prompt templates.\n",
    "- `mock_runner`: Simulates a model response (replace with actual model call for real use).\n",
    "- `run_prompt_tests`: Runs all prompt variations and collects responses.\n",
    "\n",
    "The following code cell demonstrates prompt debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d0ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Translate into formal English: 'Bonjour'\n",
      "Response: Response to: Translate into formal English: 'Bonjour'\n",
      "---\n",
      "Prompt: Translate into formal English: 'Comment ça va?'\n",
      "Response: Response to: Translate into formal English: 'Comment ça va?'\n",
      "---\n",
      "Prompt: Translate into casual English: 'Bonjour'\n",
      "Response: Response to: Translate into casual English: 'Bonjour'\n",
      "---\n",
      "Prompt: Translate into casual English: 'Comment ça va?'\n",
      "Response: Response to: Translate into casual English: 'Comment ça va?'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import itertools\n",
    "\n",
    "\n",
    "# 1. Generate prompt variations\n",
    "def generate_variations(\n",
    "    base_prompt: str, variations: Dict[str, List[str]]\n",
    ") -> List[str]:\n",
    "    keys = list(variations.keys())\n",
    "    combos = list(itertools.product(*(variations[k] for k in keys)))\n",
    "    prompts = []\n",
    "    for combo in combos:\n",
    "        p = base_prompt\n",
    "        for k, v in zip(keys, combo):\n",
    "            p = p.replace(\"{\" + k + \"}\", v)\n",
    "        prompts.append(p)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Mock runner (replace with OpenAI/HF runner)\n",
    "def mock_runner(prompt: str) -> str:\n",
    "    return \"Response to: \" + prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b72187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run tests\n",
    "def run_prompt_tests(prompts: List[str]):\n",
    "    results = []\n",
    "    for p in prompts:\n",
    "        resp = mock_runner(p)\n",
    "        results.append({\"prompt\": p, \"response\": resp})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef4eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage 1\n",
    "base = \"Translate into {tone} English: '{sentence}'\"\n",
    "vars = {\"tone\": [\"formal\", \"casual\"], \"sentence\": [\"Bonjour\", \"Comment ça va?\"]}\n",
    "\n",
    "prompts = generate_variations(base, vars)\n",
    "results = run_prompt_tests(prompts)\n",
    "\n",
    "for r in results:\n",
    "    print(\"Prompt:\", r[\"prompt\"])\n",
    "    print(\"Response:\", r[\"response\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage 2 (with different sentences)\n",
    "base2 = \"Summarize in {style} style: '{text}'\"\n",
    "vars2 = {\n",
    "    \"style\": [\"bullet\", \"paragraph\"],\n",
    "    \"text\": [\"The sun rises in the east.\", \"Water is essential for life.\"],\n",
    "}\n",
    "\n",
    "prompts2 = generate_variations(base2, vars2)\n",
    "results2 = run_prompt_tests(prompts2)\n",
    "\n",
    "for r in results2:\n",
    "    print(\"Prompt:\", r[\"prompt\"])\n",
    "    print(\"Response:\", r[\"response\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb517232",
   "metadata": {},
   "source": [
    "# Theory: Performance Evaluation Metrics\n",
    "\n",
    "Performance metrics help evaluate the quality of generated text compared to reference outputs.\n",
    "\n",
    "- **Accuracy**: Measures exact match between prediction and reference.\n",
    "- **BLEU**: Evaluates n-gram overlap (commonly used for translation).\n",
    "- **ROUGE**: Measures overlap of sequences (used for summarization).\n",
    "- **Semantic Similarity**: Measures meaning similarity using embeddings.\n",
    "\n",
    "The following code cell defines functions for each metric and demonstrates their usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0389fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "BLEU: 100.00000000000004\n",
      "ROUGE: [{'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a51bc1d87c4ca8afe879b1dc598d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36af73b086cb47cf9aefe5ce0b71e85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08117ce7b5694259b93a3d1c59052599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f415244c713e45a29d3bc9a1c6a1a583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9626a4176f6642689078dabaea418d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e36b6085fd1438b9774bfb379466572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9b1e6d808b48f68c19b73bd3662185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff27acc74394165bad78b8f6d37f8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9a2ce1f49844d58509d5ddba8fe028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e508e4336c493795d8fe83399c94a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db002011407e435b8ba39e6fbb06e504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity: [1.0000001192092896, 1.000000238418579]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# 1. Accuracy / Exact Match\n",
    "def accuracy(preds: List[str], refs: List[str]) -> float:\n",
    "    return sum([p.strip() == r.strip() for p, r in zip(preds, refs)]) / len(preds)\n",
    "\n",
    "\n",
    "# 2. BLEU Score\n",
    "def bleu(preds: List[str], refs: List[str]) -> float:\n",
    "    return sacrebleu.corpus_bleu(preds, [refs]).score\n",
    "\n",
    "\n",
    "# 3. ROUGE Scores\n",
    "def rouge(preds: List[str], refs: List[str]):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = [scorer.score(r, p) for p, r in zip(preds, refs)]\n",
    "    return scores\n",
    "\n",
    "\n",
    "# 4. Semantic Similarity\n",
    "def semantic_similarity(preds: List[str], refs: List[str]) -> List[float]:\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    pred_emb = model.encode(preds, convert_to_tensor=True)\n",
    "    ref_emb = model.encode(refs, convert_to_tensor=True)\n",
    "    sims = util.cos_sim(pred_emb, ref_emb)\n",
    "    return [float(sims[i, i]) for i in range(len(preds))]\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "preds = [\"Hello\", \"How are you?\"]\n",
    "refs = [\"Hello\", \"How are you?\"]\n",
    "\n",
    "print(\"Accuracy:\", accuracy(preds, refs))\n",
    "print(\"BLEU:\", bleu(preds, refs))\n",
    "print(\"ROUGE:\", rouge(preds, refs))\n",
    "print(\"Semantic similarity:\", semantic_similarity(preds, refs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f422a9b",
   "metadata": {},
   "source": [
    "# Theory: Example Metric Calculation\n",
    "\n",
    "This cell demonstrates how to use the installed libraries to calculate BLEU, ROUGE, and semantic similarity scores for sample outputs.\n",
    "\n",
    "- BLEU compares n-gram overlap.\n",
    "- ROUGE compares sequence overlap.\n",
    "- Semantic similarity uses embeddings to compare meaning.\n",
    "\n",
    "You can modify the reference and candidate outputs for your own evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a99823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 57.89300674674101\n",
      "ROUGE-1: 0.9090909090909091\n",
      "ROUGE-L: 0.9090909090909091\n",
      "Cosine Similarity: 0.992526650428772\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Reference and candidate outputs\n",
    "reference = [\"The cat is on the mat\"]\n",
    "candidate = \"The cat is on mat\"\n",
    "\n",
    "# BLEU Score\n",
    "bleu = sacrebleu.corpus_bleu([candidate], [reference])\n",
    "print(\"BLEU:\", bleu.score)\n",
    "\n",
    "# ROUGE Score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "scores = scorer.score(reference[0], candidate)\n",
    "print(\"ROUGE-1:\", scores[\"rouge1\"].fmeasure)\n",
    "print(\"ROUGE-L:\", scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "# Semantic Similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "ref_emb = model.encode(reference[0], convert_to_tensor=True)\n",
    "cand_emb = model.encode(candidate, convert_to_tensor=True)\n",
    "similarity = util.cos_sim(ref_emb, cand_emb)\n",
    "print(\"Cosine Similarity:\", float(similarity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
