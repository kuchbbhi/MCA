{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65208962",
   "metadata": {},
   "source": [
    "## Lexical Ambiguity (Word Meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5fff80",
   "metadata": {},
   "source": [
    "##### 1. Rule Based (Lesk Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4383613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "# Downloads needed for Lesk + tokenization\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab') # This is needed in newer versions of nltk. If not used we get a warning. \n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "STOPWORD = set(stopwords.words('english'))\n",
    "\n",
    "def lesk(word, sentence):\n",
    "    words = nltk.word_tokenize(sentence.lower())    \n",
    "    best_sense, max_overlap = None, 0\n",
    "    \n",
    "    for synonym in wordnet.synsets(word): # Synonym sets contains all possible meanings of a word\n",
    "        '''we are creating a list of words that define the meaning of the synonym.\n",
    "            This includes :\n",
    "              1. definition\n",
    "              2. lemma names\n",
    "              3. examples\n",
    "              \n",
    "            Then we make all words lowercase and remove stopwords and non-alphabetic words.\n",
    "            '''\n",
    "        meaning_words = synonym.definition().split() + synonym.lemma_names() + synonym.examples()\n",
    "        \n",
    "        final_meaning_words = []\n",
    "        for w in meaning_words:\n",
    "            if w.isalpha() and w.lower() not in STOPWORD:\n",
    "                final_meaning_words.append(w.lower())\n",
    "        \n",
    "        overlap = len(set(final_meaning_words) & set(words))\n",
    "\n",
    "        if overlap > max_overlap:\n",
    "            best_sense = synonym\n",
    "            max_overlap = overlap\n",
    "    return best_sense\n",
    "\n",
    "sent1 = \"He went to the bank to deposit money.\"\n",
    "sent2 = \"They sat on the bank of the river.\"\n",
    "\n",
    "print(\"Sentence 1:\", lesk(\"bank\", sent1))\n",
    "print(\"Sentence 2:\", lesk(\"bank\", sent2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e0c1e",
   "metadata": {},
   "source": [
    "##### 2. Statistical (Naive Bayes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Inputs\n",
    "sentences = [\"I deposited money in the bank\", \n",
    "                 \"She took a loan from the bank\", \n",
    "                 \"They sat on the river bank\", \n",
    "                 \"The river bank was flooded\"]\n",
    "labels = [\"finance\", \"finance\", \"river\", \"river\"]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "sentence_vector = vectorizer.fit_transform(sentences)\n",
    "\n",
    "model = MultinomialNB().fit(sentence_vector, labels)\n",
    "\n",
    "print(model.predict(vectorizer.transform([\"deposit at bank\"])))\n",
    "print(model.predict(vectorizer.transform([\"walked by river bank\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269396d",
   "metadata": {},
   "source": [
    "## Syntactic Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac413ad",
   "metadata": {},
   "source": [
    "##### 1. Rule Based (CFG - Context Free Grammer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import CFG, ChartParser\n",
    "\n",
    "# We are defining our own Grammar Rules\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "# Sentence\n",
    "S -> NP VP\n",
    "\n",
    "# Noun Phrase\n",
    "NP -> 'I'\n",
    "NP -> 'the' N\n",
    "NP -> 'the' N PP\n",
    "\n",
    "# Verb Phrase\n",
    "VP -> V NP\n",
    "VP -> VP PP\n",
    "\n",
    "# Prepositional Phrase\n",
    "PP -> P NP\n",
    "\n",
    "# Nouns\n",
    "N -> 'man'\n",
    "N -> 'telescope'\n",
    "\n",
    "# Verbs\n",
    "V -> 'saw'\n",
    "\n",
    "# Prepositions\n",
    "P -> 'with'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Sentence to parse\n",
    "sentence = \"I saw the man with the telescope\".split()\n",
    "\n",
    "# Parse using rules\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "for tree in parser.parse(sentence):\n",
    "    tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb319ef5",
   "metadata": {},
   "source": [
    "##### 2. Statistical (PCFG - Probabilistic Context Free Grammer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PCFG, ViterbiParser\n",
    "\n",
    "# We are defining our own Grammar Rules\n",
    "probabilistic_grammar = PCFG.fromstring(\"\"\"\n",
    "# Sentence\n",
    "S -> NP VP [1.0]\n",
    "\n",
    "# Noun Phrase\n",
    "NP -> 'I' [0.3]\n",
    "NP -> 'the' N [0.4]\n",
    "NP -> 'the' N PP [0.3]\n",
    "\n",
    "# Verb Phrase\n",
    "VP -> V NP [0.4]\n",
    "VP -> VP PP [0.6]\n",
    "\n",
    "# Prepositional Phrase\n",
    "PP -> P NP [1.0]\n",
    "\n",
    "# Nouns\n",
    "N -> 'man' [0.5]\n",
    "N -> 'telescope' [0.5]\n",
    "\n",
    "# Verbs\n",
    "V -> 'saw' [1.0]\n",
    "\n",
    "# Prepositions\n",
    "P -> 'with' [1.0]\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Sentence to parse\n",
    "sentence = \"I saw the man with the telescope\".split()\n",
    "\n",
    "# Parse using Probability Rules\n",
    "parser = ViterbiParser(probabilistic_grammar)\n",
    "\n",
    "for tree in parser.parse(sentence):\n",
    "    tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854955e",
   "metadata": {},
   "source": [
    "## Semantic Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6f2a9",
   "metadata": {},
   "source": [
    "##### 1. Rule Based (Using POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d55fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Visiting', 'VBG'), ('relatives', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('annoying', 'VBG'), ('.', '.')]\n",
      "Rule-Based: Meaning: The act of visiting relatives is annoying.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "sentence = \"Visiting relatives can be annoying.\"\n",
    "\n",
    "def semantic_rule(sentence):\n",
    "    words = nltk.word_tokenize(sentence)          \n",
    "    pos = nltk.pos_tag(words)\n",
    "\n",
    "    # If first word is a verb (V), then \"visiting\" is an action\n",
    "    if pos[0][1].startswith('V'):                 \n",
    "        return \"Meaning: The act of visiting relatives is annoying.\"\n",
    "    else:\n",
    "        return \"Meaning: Relatives who are visiting are annoying.\"\n",
    "\n",
    "print(\"Rule-Based:\", semantic_rule(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab599a",
   "metadata": {},
   "source": [
    "##### 2. Statistical (Cue Based Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d1e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cue Method 1: Not sure / ambiguous\n",
      "Cue Method 2: People sense (relatives who visit).\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Clue word lists for meaning detection\n",
    "cue_activity = {\"annoying\" , \"going\", \"travel\", \"trip\", \"activity\"}\n",
    "cue_people = {\"relatives\", \"family\", \"friend\", \"guest\", \"visitor\"}\n",
    "\n",
    "\n",
    "def cue_disambiguate(sentence):\n",
    "    words = set(w.lower() for w in nltk.word_tokenize(sentence))\n",
    "\n",
    "    score_activity = len(words & cue_activity)  # Common words with activity-related words\n",
    "    score_people = len(words & cue_people)      # Common words with people-related words\n",
    "\n",
    "    if score_activity > score_people:\n",
    "        return \"Activity sense (the act of visiting).\"\n",
    "    elif score_people > score_activity:\n",
    "        return \"People sense (relatives who visit).\"\n",
    "    else:\n",
    "        return \"Not sure / ambiguous\"\n",
    "\n",
    "print(\"Cue Method 1:\", cue_disambiguate(\"Visiting relatives can be annoying.\"))\n",
    "print(\"Cue Method 2:\", cue_disambiguate(\"Relatives visiting the city can be noisy.\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
