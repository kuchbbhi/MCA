{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcWxj2jqWvZ9"
      },
      "source": [
        "\n",
        "**IMPORTING REQUIRED LIBRARIES**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYQno_15WlgC"
      },
      "outputs": [],
      "source": [
        "import nltk          # NLP toolkit for tokenization\n",
        "import re            # Regex for word extraction\n",
        "from nltk.util import ngrams   # To generate unigrams, bigrams, trigrams\n",
        "from collections import Counter # Efficient word frequency counter\n",
        "import math          # For log, exp in perplexity calculation\n",
        "import matplotlib.pyplot as plt # For plotting frequency distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8I9av4kW7q2"
      },
      "source": [
        "\n",
        "**TOKENIZATION FUNCTION**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1xxYn-yW4B-"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    nltk.download(\"punkt\", quiet=True)      # Punkt tokenizer (pre-trained)\n",
        "    nltk.download(\"punkt_tab\", quiet=True)  # Needed in some NLTK versions\n",
        "    from nltk import word_tokenize\n",
        "\n",
        "    def tokenize(text):\n",
        "        return word_tokenize(text.lower())\n",
        "\n",
        "except:\n",
        "    # If NLTK fails, use regex: \\b\\w+\\b matches full words only.\n",
        "    def tokenize(text):\n",
        "        return re.findall(r\"\\b\\w+\\b\", text.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXbetr1ZXQfy"
      },
      "source": [
        "\n",
        "**SAMPLE TEXT CORPUS**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89KaraPIXIPk"
      },
      "outputs": [],
      "source": [
        "# A corpus = collection of text used to build models.\n",
        "# This is a very small toy dataset (normally, corpora contain millions of words).\n",
        "text = \"\"\"The dog barks. The cat meows. The dog runs fast.\n",
        "The cat sleeps. The dog eats food. The cat drinks milk.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxLLeSJhXpEI"
      },
      "source": [
        "**TOKENIZATION STEP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21iVqKReXqoB"
      },
      "outputs": [],
      "source": [
        "tokens = tokenize(text)\n",
        "# tokens = [\"the\", \"dog\", \"barks\", \"the\", \"cat\", \"meows\", ...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr-LC8j-X1Qw"
      },
      "source": [
        "**BUILDING N-GRAMS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09n8GZhCX3hj"
      },
      "outputs": [],
      "source": [
        "# Unigram  → sequence of 1 word  (useful for word frequency analysis)\n",
        "# Bigram   → sequence of 2 words (captures immediate context)\n",
        "# Trigram  → sequence of 3 words (captures more context)\n",
        "#\n",
        "# Example:\n",
        "#   \"The dog barks\"\n",
        "#   - Unigrams: (\"the\"), (\"dog\"), (\"barks\")\n",
        "#   - Bigrams : (\"the\",\"dog\"), (\"dog\",\"barks\")\n",
        "#   - Trigrams: (\"the\",\"dog\",\"barks\")\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av4jg6XsYY0Y"
      },
      "source": [
        "**FREQUENCY COUNTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_rm3bV_YYi0"
      },
      "outputs": [],
      "source": [
        "# Counting how many times each n-gram appears in the corpus.\n",
        "# This is required to compute probabilities:\n",
        "#   P(w2 | w1) = count(w1, w2) / count(w1)\n",
        "unigram_freq = Counter(unigrams)\n",
        "bigram_freq = Counter(bigrams)\n",
        "trigram_freq = Counter(trigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f6k2oE6YgXj"
      },
      "source": [
        "**NEXT WORD PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_N6SKrdYgFv"
      },
      "outputs": [],
      "source": [
        "def predict_next_bigram(word):\n",
        "    # Collect candidate words that follow the given word\n",
        "    candidates = {w2: count for (w1, w2), count in bigram_freq.items() if w1 == word}\n",
        "    if not candidates:\n",
        "        return None\n",
        "    # Return the word with maximum probability (highest frequency)\n",
        "    return max(candidates, key=candidates.get)\n",
        "\n",
        "def predict_next_trigram(w1, w2):\n",
        "    candidates = {w3: count for (x1, x2, w3), count in trigram_freq.items() if x1 == w1 and x2 == w2}\n",
        "    if not candidates:\n",
        "        return None\n",
        "    return max(candidates, key=candidates.get)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjmqADVnYnh0"
      },
      "source": [
        "**PERPLEXITY CALCULATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCvQVDagYnPL"
      },
      "outputs": [],
      "source": [
        "def perplexity(test_tokens, model=\"bigram\"):\n",
        "    N = len(test_tokens)  # total words\n",
        "    log_prob = 0\n",
        "\n",
        "    if model == \"bigram\":\n",
        "        for i in range(1, N):\n",
        "            w1, w2 = test_tokens[i-1], test_tokens[i]\n",
        "            prob = (bigram_freq[(w1, w2)] + 1) / (unigram_freq[(w1,)] + len(unigram_freq))\n",
        "            log_prob += math.log(prob)\n",
        "\n",
        "    elif model == \"trigram\":\n",
        "        for i in range(2, N):\n",
        "            w1, w2, w3 = test_tokens[i-2], test_tokens[i-1], test_tokens[i]\n",
        "            prob = (trigram_freq[(w1, w2, w3)] + 1) / (bigram_freq[(w1, w2)] + len(unigram_freq))\n",
        "            log_prob += math.log(prob)\n",
        "\n",
        "    return math.exp(-log_prob / N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bpQV6xEYubE"
      },
      "source": [
        "**VISUALIZATION FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AX-7VhTYuMF"
      },
      "outputs": [],
      "source": [
        "def plot_top_ngrams(freq_dict, title, n=10):\n",
        "    # Convert tuple keys like ('the','dog') → \"the dog\"\n",
        "    ngrams_list = [\" \".join(gram) for gram, _ in freq_dict.most_common(n)]\n",
        "    counts = [count for _, count in freq_dict.most_common(n)]\n",
        "\n",
        "    # Plot bar chart\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.bar(ngrams_list, counts, color=\"skyblue\", edgecolor=\"black\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"N-grams\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi09JMsAY2HI"
      },
      "source": [
        "**EXAMPLE OUTPUTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrumBokvY0-H"
      },
      "outputs": [],
      "source": [
        "print(\"Tokens:\", tokens)  # Show tokenized words\n",
        "print(\"Bigram Prediction after 'dog':\", predict_next_bigram(\"dog\"))  # Predict next word after \"dog\"\n",
        "print(\"Trigram Prediction after 'the dog':\", predict_next_trigram(\"the\", \"dog\"))  # Predict after \"the dog\"\n",
        "\n",
        "print(\"Bigram Perplexity:\", perplexity(tokens, model=\"bigram\"))  # Evaluate model performance\n",
        "print(\"Trigram Perplexity:\", perplexity(tokens, model=\"trigram\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJWSkGFqY7se"
      },
      "source": [
        "**VISUALIZATION OF TOP N-GRAMS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoJbqsDBY9YU"
      },
      "outputs": [],
      "source": [
        "plot_top_ngrams(unigram_freq, \"Top Unigrams\", n=10)\n",
        "\n",
        "# Top 10 most frequent word pairs\n",
        "plot_top_ngrams(bigram_freq, \"Top Bigrams\", n=10)\n",
        "\n",
        "# Top 10 most frequent word triples\n",
        "plot_top_ngrams(trigram_freq, \"Top Trigrams\", n=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
